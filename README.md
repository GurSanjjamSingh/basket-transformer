# ğŸ›’ Basket Transformer
*A lightweight transformer model for predicting the next grocery product in a basket sequence.*

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)  
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-orange.svg)](https://pytorch.org/)
[![Apple Silicon](https://img.shields.io/badge/Apple%20Silicon-Optimized-green.svg)](https://developer.apple.com/metal/)

---

## ğŸ¯ Project Overview

This project demonstrates how transformer architectures can be adapted for grocery basket sequence prediction. Using a custom GPT-style model, we predict the next product a customer is likely to add to their basket based on their current items.

**Key Features:**
- ğŸš€ **Optimized for Apple Silicon** (M1/M2/M3 Macs)
- ğŸ“Š **Balanced training data** with smart down-sampling
- âš¡ **Fast training** (~2 hours on M2 Pro)
- ğŸ¯ **Real-world performance** with interpretable results

---

## ğŸ§  Technical Architecture

### Data Pipeline
The project includes a sophisticated data preprocessing pipeline:

1. **Data Shrinking**: Reduces vocabulary from 92K â†’ 5K most frequent products
2. **Balanced Sampling**: Down-samples popular items (20% kept) while preserving long-tail diversity
3. **Smart Mapping**: Remaps product IDs to create efficient vocabulary
4. **Target Sizing**: Produces ~400K training / ~50K validation samples

### Model Configuration

**Smoke Test (15 minutes):**
```python
emb_dim=128, n_heads=8, n_layers=3, batch_size=128
context_length=25, vocab_size=5_000
```

**Full Training (~2 hours on M2 Pro):**
```python
emb_dim=128, n_heads=8, n_layers=3, batch_size=96
context_length=30, vocab_size=5_000, epochs=5
```

**Advanced Features:**
- âœ… **Learning Rate Warmup** with cosine decay
- âœ… **Gradient Clipping** (norm=1.0) 
- âœ… **AdamW Optimizer** with proper weight decay
- âœ… **Layer Normalization** (pre-norm architecture)
- âœ… **Early Stopping** with validation monitoring
- âœ… **Smart Checkpointing** for long training runs

### Training Process

The model uses next-token prediction with cross-entropy loss:

1. **Sequence Processing**: Baskets â†’ product ID sequences (max length 30)
2. **Causal Masking**: Prevents future token leakage
3. **Loss Calculation**: Cross-entropy on vocabulary predictions
4. **Optimization**: AdamW with lr warmup + cosine decay

---

## ğŸ“Š Results & Performance

### Training Metrics
- **Training Time**: ~70 minutes (M2 Pro)
- **Memory Usage**: ~2-3GB peak
- **Convergence**: Stable loss decrease within 2-3 epochs
- **Model Size**: ~1M parameters

### Prediction Quality
The model demonstrates strong sequential learning:

**Example Prediction:**
```
Current Basket: ['ORGANIC BANANAS', 'GREEK YOGURT', 'WHOLE MILK']
Top-3 Predictions: ['BREAD', 'EGGS', 'CHEESE']
```

**Performance Indicators:**
- âœ… Coherent product relationships (dairy â†’ bread/eggs)
- âœ… No repetition of current basket items
- âœ… Reasonable category associations
- âœ… Stable training without overfitting

---

## ğŸš€ Quick Start

### Prerequisites
```bash
# Required packages
pip install torch pandas pyarrow tqdm matplotlib
```

### 1. Data Preparation
```bash
# Download Dunnhumby dataset to data/ directory
python data_preprocessing.py    # Creates splits
python shrink_bal.py           # Balances & shrinks dataset
```

### 2. Model Training
```bash
# Smoke test (15 min)
python train.py --config smoke_test_cfg.py

# Full training (2 hours)  
python train.py --config full_cfg.py
```

### 3. Inference
```bash
python predict.py --model checkpoints/best_model.pt
```

---

## ğŸ“š Dataset

**Source**: [Dunnhumby - The Complete Journey](https://www.kaggle.com/datasets/frtgnn/dunnhumby-the-complete-journey)

**Original Scale:**
- 2M+ baskets from 2,500 households
- 92,341 unique products
- 2 years of transaction history

**Processed Scale:**
- 400K training baskets
- 50K validation baskets  
- 5K most frequent products
- Balanced head/tail distribution

### Key Files
```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ product.csv              # Product catalog
â”‚   â””â”€â”€ transaction_data.csv     # Raw transactions
â”œâ”€â”€ splits/
â”‚   â”œâ”€â”€ train.parquet           # Training sequences
â”‚   â””â”€â”€ val.parquet             # Validation sequences
â””â”€â”€ shrink_bal/
    â”œâ”€â”€ train.parquet           # Balanced training data
    â”œâ”€â”€ val.parquet             # Balanced validation data
    â””â”€â”€ id2product.json         # Product mapping
```

---

## ğŸ› ï¸ Project Structure

```
basket-transformer/
â”œâ”€â”€ data_preprocessing.py       # Raw data â†’ sequences
â”œâ”€â”€ shrink_bal.py              # Vocabulary shrinking & balancing
â”œâ”€â”€ smoke_test_cfg.py          # Quick test configuration
â”œâ”€â”€ full_cfg.py                # Full training configuration  
â”œâ”€â”€ model.py                   # Transformer implementation
â”œâ”€â”€ train.py                   # Training loop
â”œâ”€â”€ predict.py                 # Inference & examples
â”œâ”€â”€ utils.py                   # Helper functions
â””â”€â”€ README.md                  # This file
```

---

## ğŸ’¡ Key Innovations

### 1. **Smart Data Balancing**
Instead of naive sampling, we:
- Identify actual top-3 frequent items after vocabulary remapping
- Down-sample only these items (20% kept)
- Preserve all long-tail diversity
- Apply global caps to meet target dataset sizes

### 2. **Apple Silicon Optimization**
- MPS backend compatibility (no mixed precision)
- Memory-efficient batch sizes
- Conservative model sizing for 2-hour training constraint
- Proper gradient clipping for MPS stability

### 3. **Production-Ready Training**
- Learning rate warmup prevents early instability
- Cosine decay maintains performance through training
- Early stopping prevents overfitting
- Smart checkpointing for recovery

---

## ğŸ¯ Future Improvements

- [ ] **Larger Models**: Scale to 6-12 layers with more GPU/memory
- [ ] **Advanced Architectures**: Try RoPE, Flash Attention, or Mamba
- [ ] **Better Features**: Include price, category, seasonality data
- [ ] **Evaluation Metrics**: Implement BLEU, diversity metrics
- [ ] **Production Deployment**: Add inference API and model serving

---

